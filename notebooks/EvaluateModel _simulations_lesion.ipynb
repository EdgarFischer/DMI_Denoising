{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "600129ff",
   "metadata": {},
   "source": [
    "# Visualisierung eines Noise2Void-Runs\n",
    "\n",
    "In diesem Notebook gibst du **einmalig** den Pfad zu deinem gespeicherten Run-Ordner an.  \n",
    "Dann werden daraus automatisch geladen:\n",
    "- die `config.py` (aus `used_source`),  \n",
    "- dein U-Net-Modell mit dem Checkpoint `best.pt`,  \n",
    "- Trainings- und Validierungs-Datasets,  \n",
    "- und es werden exemplarisch je drei Beispiele aus Training und Validation geplottet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20936a",
   "metadata": {},
   "source": [
    "# Parameter setzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2259ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# — Stelle hier deinen Run-Ordner ein (z.B. …/trained_models/First_Test) —\n",
    "run_dir = \"../trained_models/Lesion_double_Ynet_continued\"    #_3Layers_ynetRank3 <-- anpassen\n",
    "\n",
    "# GPU wählen\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or whichever GPU you want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4f9f1d",
   "metadata": {},
   "source": [
    "# Andere parameter automatisch laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "058209dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⤷ sys.path[0:2] = ['/workspace/Deuterium_Denosing/trained_models/Lesion_double_Ynet_continued/used_source', '/workspace/Deuterium_Denosing']\n",
      "RUN_NAME           = Lesion_double_Ynet_continued\n",
      "Checkpoint-Ordner  = /workspace/Deuterium_Denosing/trained_models/Lesion_double_Ynet_continued/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# ── Zelle 2: Run-Config & used_source in sys.path einfügen ──\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib.util\n",
    "\n",
    "# 1) Absoluter Pfad zum Run-Verzeichnis (aus Zelle 1)\n",
    "run_dir = os.path.abspath(run_dir)\n",
    "\n",
    "# 2) Pfad zum used_source-Snapshot\n",
    "USED_SRC = os.path.join(run_dir, \"used_source\")\n",
    "\n",
    "# 3) Ganz vorne in sys.path einfügen, damit alle Imports daraus gezogen werden\n",
    "if USED_SRC not in sys.path:\n",
    "    sys.path.insert(0, USED_SRC)\n",
    "\n",
    "# (Optional) Projekt-Root als Fallback auf Position 1\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(1, PROJECT_ROOT)\n",
    "\n",
    "# Debug: sys.path prüfen\n",
    "print(\"⤷ sys.path[0:2] =\", sys.path[:2])\n",
    "\n",
    "# 4) Pfad zur Config im used_source-Ordner\n",
    "config_path = os.path.join(USED_SRC, \"config.py\")\n",
    "\n",
    "# 5) Dynamisch als Modul importieren\n",
    "spec = importlib.util.spec_from_file_location(\"run_config\", config_path)\n",
    "run_config = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(run_config)\n",
    "\n",
    "# 6) Absoluten Pfad für checkpoint_dir anpassen (falls relativ)\n",
    "if not os.path.isabs(run_config.checkpoint_dir):\n",
    "    run_config.checkpoint_dir = os.path.join(PROJECT_ROOT, run_config.checkpoint_dir)\n",
    "\n",
    "# 7) Werte ausgeben\n",
    "print(f\"RUN_NAME           = {run_config.RUN_NAME}\")\n",
    "print(f\"Checkpoint-Ordner  = {run_config.checkpoint_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314d65f",
   "metadata": {},
   "source": [
    "# Bibliotheken und Geräte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b24ae",
   "metadata": {},
   "source": [
    "# Model laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277f3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verwendetes Device: cuda\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspace/Deuterium_Denosing/trained_models/Lesion_double_Ynet_continued/checkpoints/last.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# --- Checkpoint laden --------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     50\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_config\u001b[38;5;241m.\u001b[39mcheckpoint_dir, ckpt_name)\n\u001b[0;32m---> 51\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/workspace/Deuterium_Denosing/trained_models/Lesion_double_Ynet_continued/checkpoints/last.pt'"
     ]
    }
   ],
   "source": [
    "# --- Cell A: Device & Model --------------------------------------------------\n",
    "import torch, os\n",
    "\n",
    "# Device wählen\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Verwendetes Device:\", device)\n",
    "\n",
    "# Modellklasse & Checkpoint-Name je nach TRAIN_METHOD\n",
    "tm = run_config.TRAIN_METHOD.lower()\n",
    "ud = run_config.UNET_DIM.lower()\n",
    "\n",
    "if tm == \"ynet\":\n",
    "    assert ud == \"2d\", \"Y-Net ist nur für 2-D verfügbar\"\n",
    "    from models.ynet2d import YNet2D as NetClass\n",
    "    ckpt_name = \"best_ynet.pt\"\n",
    "    model = NetClass(\n",
    "        in_ch_noisy  = run_config.in_channels_noisy,\n",
    "        in_ch_lr     = run_config.in_channels_lr,\n",
    "        out_channels = run_config.out_channels,\n",
    "        features     = run_config.features\n",
    "    ).to(device)\n",
    "\n",
    "elif tm == \"sup_lowrank\":\n",
    "    assert ud == \"2d\", \"Supervised LowRank U-Net wurde nur für 2-D implementiert\"\n",
    "    from models.unet2d import UNet2D as NetClass\n",
    "    ckpt_name = \"best_unet_lowrank.pt\"\n",
    "    model = NetClass(\n",
    "        in_channels  = run_config.in_channels,\n",
    "        out_channels = run_config.out_channels,\n",
    "        features     = run_config.features\n",
    "    ).to(device)\n",
    "\n",
    "else:\n",
    "    if ud == \"2d\":\n",
    "        from models.unet2d import UNet2D as NetClass\n",
    "        ckpt_name = \"last.pt\"\n",
    "        features = run_config.features\n",
    "    else:  # ud == \"3d\"\n",
    "        from models.unet3d import UNet3D as NetClass\n",
    "        ckpt_name = \"best3d.pt\"\n",
    "        features = run_config.features_3d  # ← hier die Änderung\n",
    "\n",
    "    model = NetClass(\n",
    "        in_channels  = run_config.in_channels,\n",
    "        out_channels = run_config.out_channels,\n",
    "        features     = features\n",
    "    ).to(device)\n",
    "\n",
    "# --- Checkpoint laden --------------------------------------------------------\n",
    "ckpt_path = os.path.join(run_config.checkpoint_dir, ckpt_name)\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Modell geladen: {run_config.TRAIN_METHOD.upper()}-{run_config.UNET_DIM}  \"\n",
    "      f\"({ckpt_name},  Epoch {ckpt.get('epoch','?')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92faba37",
   "metadata": {},
   "source": [
    "# Datasets & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b40d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell B + C: End-to-End-Inference & Post-Processing ──────────────────────\n",
    "import numpy as np, torch, os\n",
    "from itertools import product\n",
    "from math import prod\n",
    "from data.data_utils import load_and_preprocess_data, low_rank\n",
    "\n",
    "# ---------------------------------------------------------------------------#\n",
    "# 1) Validation-Daten laden\n",
    "# ---------------------------------------------------------------------------#\n",
    "data_val = load_and_preprocess_data(\n",
    "    folder_names = run_config.val_data,#run_config.val_data ,#run_config.val_data, ['P07']\n",
    "    base_path    = os.path.join(PROJECT_ROOT, \"datasets\"),\n",
    "    fourier_axes = run_config.fourier_transform_axes\n",
    ")\n",
    "\n",
    "print(np.max(np.abs(data_val)))\n",
    "\n",
    "data_gt = load_and_preprocess_data(\n",
    "    folder_names = ['Simulated_Lesion_GT_double_normalized'],#'Simulated_Lesion_GT'  , Simulated_ground_truth'\n",
    "    base_path    = os.path.join(PROJECT_ROOT, \"datasets\"),\n",
    "    fourier_axes = run_config.fourier_transform_axes\n",
    ")\n",
    "\n",
    "if run_config.TRAIN_METHOD == \"ynet\" or \"ynet_n2v\":\n",
    "    lowrank_val = low_rank(data_val.copy(), rank=run_config.lowrank_rank)\n",
    "\n",
    "# ---------------------------------------------------------------------------#\n",
    "# 2) Achsen-Definition\n",
    "# ---------------------------------------------------------------------------#\n",
    "spatial_axes = (run_config.image_axes if run_config.UNET_DIM == \"2d\"\n",
    "                else run_config.volume_axes)            # (len==2) oder (len==3)\n",
    "all_axes     = list(range(data_val.ndim))\n",
    "other_axes   = [ax for ax in all_axes if ax not in spatial_axes]\n",
    "ranges       = [range(data_val.shape[ax]) for ax in other_axes]\n",
    "\n",
    "print(f\"Spatial axes: {spatial_axes} | Other axes: {other_axes} \"\n",
    "      f\"→ geplante Vorwärtspässe: {prod(len(r) for r in ranges)}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------#\n",
    "# 3) Output-Array anlegen\n",
    "# ---------------------------------------------------------------------------#\n",
    "out_data = np.zeros_like(data_val, dtype=np.complex64)\n",
    "\n",
    "# ---------------------------------------------------------------------------#\n",
    "# 4) Hilfs-Funktion\n",
    "# ---------------------------------------------------------------------------#\n",
    "def to_tensor(arr):          # arr shape (2,*spatial)\n",
    "    return torch.from_numpy(arr)[None].to(device)\n",
    "\n",
    "# ---------------------------------------------------------------------------#\n",
    "# ---------------------------------------------------------------------------#\n",
    "# 5) Inference\n",
    "# ---------------------------------------------------------------------------#\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    if run_config.UNET_DIM.lower() == \"3d\":\n",
    "        # ─────────────────────────────── 3-D: drei Ansichten mitteln ───────────────────────────────\n",
    "        axis_triplets = [(0, 3, 4), (1, 3, 4), (2,3,4)]   # x,f,T  | y,f,T | z,f,T\n",
    "        out_sum  = np.zeros_like(data_val, dtype=np.complex64)\n",
    "        vote_cnt = np.zeros_like(data_val, dtype=np.uint8)\n",
    "\n",
    "        for spatial_axes in axis_triplets:\n",
    "            all_axes   = list(range(data_val.ndim))\n",
    "            other_axes = [ax for ax in all_axes if ax not in spatial_axes]\n",
    "            ranges     = [range(data_val.shape[ax]) for ax in other_axes]\n",
    "\n",
    "            for idx in product(*ranges):\n",
    "                slicer = [slice(None) if ax in spatial_axes\n",
    "                          else idx[other_axes.index(ax)]\n",
    "                          for ax in all_axes]\n",
    "\n",
    "                spec = data_val[tuple(slicer)]                  # (spatial,F,T) komplex\n",
    "                img  = np.stack([spec.real, spec.imag], axis=0).astype(np.float32)\n",
    "                pred = model(to_tensor(img)).squeeze(0).cpu().numpy()\n",
    "\n",
    "                den = pred[0] + 1j * pred[1]\n",
    "                out_sum [tuple(slicer)] += den\n",
    "                vote_cnt[tuple(slicer)] += 1\n",
    "\n",
    "        out_data = out_sum / np.where(vote_cnt == 0, 1, vote_cnt)\n",
    "        print(\"✓ 3-D-Inference für alle drei Ansichten abgeschlossen und gemittelt\")\n",
    "\n",
    "    else:\n",
    "        # ─────────────────────────────── 2-D: unverändert ───────────────────────────────\n",
    "        spatial_axes = run_config.image_axes\n",
    "        all_axes     = list(range(data_val.ndim))\n",
    "        other_axes   = [ax for ax in all_axes if ax not in spatial_axes]\n",
    "        ranges       = [range(data_val.shape[ax]) for ax in other_axes]\n",
    "\n",
    "        for idx in product(*ranges):\n",
    "            slicer = [slice(None) if ax in spatial_axes\n",
    "                      else idx[other_axes.index(ax)]\n",
    "                      for ax in all_axes]from scipy.io import loadmat, savemat\n",
    "\n",
    "### CAREFUL DISTINGUISH BETWEEN LESION AND NO LESION\n",
    "\n",
    "# Lade Par und mask aus der B0_1.mat-Datei\n",
    "b0_data = loadmat('../datasets/Archiv/Simulated_Lesion_GT/Lesion_120pts.mat')  #Simulated_Lesion_GT/Lesion_120pts.mat  Simulated_ground_truth/B0_1.mat\n",
    "par = b0_data['csi_data_lr']['Par'][0, 0]\n",
    "mask = b0_data['csi_data_lr']['mask'][0, 0]\n",
    "\n",
    "# Deine Datensätze\n",
    "data_dicts = {\n",
    "    'Lesion_LR8.mat': baseline_data,\n",
    "    'Lesion_noisy.mat': noisy_data,\n",
    "    'Lesion_deep.mat': out_data,\n",
    "    'Lesion_gt.mat': tgt_data\n",
    "}\n",
    "\n",
    "# Für jede Datei speichern\n",
    "for filename, data in data_dicts.items():\n",
    "    savemat(filename, {\n",
    "        'csi_data_lr': {\n",
    "            'Data': data,\n",
    "            'Par': par,\n",
    "            'mask': mask\n",
    "        }\n",
    "    })\n",
    "\n",
    "            spec = data_val[tuple(slicer)]                      # komplex-Block\n",
    "            img  = np.stack([spec.real, spec.imag], axis=0).astype(np.float32)\n",
    "\n",
    "            if run_config.TRAIN_METHOD in (\"ynet\", \"ynet_n2v\"):\n",
    "                spec_lr = lowrank_val[tuple(slicer)]\n",
    "                img_lr  = np.stack([spec_lr.real, spec_lr.imag], axis=0).astype(np.float32)\n",
    "                pred = model(to_tensor(img), to_tensor(img_lr)).squeeze(0).cpu().numpy()\n",
    "            else:\n",
    "                pred = model(to_tensor(img)).squeeze(0).cpu().numpy()\n",
    "\n",
    "            den = pred[0] + 1j * pred[1]\n",
    "            out_data[tuple(slicer)] = den\n",
    "\n",
    "        print(\"✓ 2-D-Inference abgeschlossen\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------#\n",
    "# 6) Post-Processing\n",
    "# ---------------------------------------------------------------------------#\n",
    "# Runs-Achse (letzte) ggf. entfernen\n",
    "\n",
    "tgt_data = data_gt[..., 0] if data_gt.shape[-1] == 1 else data_gt#data_gt*Normierungs_Faktor_ground_truth/Normierungs_Faktor_noisy#data_val[..., 0] if out_data.shape[-1] == 1 else data_val\n",
    "\n",
    "noisy_data = data_val[..., 0] if out_data.shape[-1] == 1 else data_val\n",
    "\n",
    "out_data = out_data[..., 0] if out_data.shape[-1] == 1 else out_data\n",
    "\n",
    "# Inverse FFT aus Frequenz-/Zeit-Domäne zurück in FID\n",
    "axes = tuple(run_config.fourier_transform_axes)\n",
    "out_data = np.fft.ifftn(np.fft.ifftshift(out_data, axes=axes), axes=axes)\n",
    "noisy_data = np.fft.ifftn(np.fft.ifftshift(noisy_data, axes=axes), axes=axes)\n",
    "tgt_data = np.fft.ifftn(np.fft.ifftshift(tgt_data, axes=axes), axes=axes)\n",
    "\n",
    "# Low-Rank-Baseline (rein qualitativ)\n",
    "rank_post = 8\n",
    "\n",
    "baseline_data = low_rank(noisy_data, rank=rank_post)\n",
    "#out_data = low_rank(out_data, rank=rank_post) \n",
    "\n",
    "# Gemeinsame Normierung (optional fürs Plotten)\n",
    "# max_val = max(np.abs(out_data).max(),\n",
    "#               np.abs(tgt_data).max(),\n",
    "#               np.abs(baseline_data).max())\n",
    "# out_data      /= np.abs(out_data).max()\n",
    "# tgt_data      /= np.abs(tgt_data).max()\n",
    "# baseline_data /= np.abs(baseline_data).max()\n",
    "\n",
    "# FFT zurück in Spektralraum für Peaks\n",
    "out_data_ft      = np.fft.fftshift(np.fft.fft(out_data, axis=3), axes=3)\n",
    "tgt_data_ft      = np.fft.fftshift(np.fft.fft(tgt_data, axis=3), axes=3)\n",
    "baseline_data_ft = np.fft.fftshift(np.fft.fft(baseline_data, axis=3), axes=3)\n",
    "noisy_data_ft = np.fft.fftshift(np.fft.fft(noisy_data, axis=3), axes=3)\n",
    "\n",
    "print(\"✓ Post-Processing fertig:\")\n",
    "print(\"  out_data      :\", out_data.shape)\n",
    "print(\"  tgt_data      :\", tgt_data.shape)\n",
    "print(\"  baseline_data :\", baseline_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e7c2e",
   "metadata": {},
   "source": [
    "# check if model uses both paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f424cf5",
   "metadata": {},
   "source": [
    "# Optional als matlab datei speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat, savemat\n",
    "\n",
    "### CAREFUL DISTINGUISH BETWEEN LESION AND NO LESION\n",
    "\n",
    "# Lade Par und mask aus der B0_1.mat-Datei\n",
    "b0_data = loadmat('../datasets/Archiv/Simulated_Lesion_GT/Lesion_120pts.mat')  #Simulated_Lesion_GT/Lesion_120pts.mat  Simulated_ground_truth/B0_1.mat\n",
    "par = b0_data['csi_data_lr']['Par'][0, 0]\n",
    "mask = b0_data['csi_data_lr']['mask'][0, 0]\n",
    "\n",
    "# Deine Datensätze\n",
    "data_dicts = {\n",
    "    'Lesion_LR8.mat': baseline_data,\n",
    "    'Lesion_noisy.mat': noisy_data,\n",
    "    'Lesion_double_Ynet.mat': out_data,\n",
    "    'Lesion_gt.mat': tgt_data\n",
    "}\n",
    "\n",
    "# Für jede Datei speichern\n",
    "for filename, data in data_dicts.items():\n",
    "    savemat(filename, {\n",
    "        'csi_data_lr': {\n",
    "            'Data': data,\n",
    "            'Par': par,\n",
    "            'mask': mask\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a8f9e",
   "metadata": {},
   "source": [
    "# Compare FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ── Einstellungen -----------------------------------------------------------\n",
    "t, T = 15, 7\n",
    "n_slices = out_data.shape[2]\n",
    "\n",
    "# ── 1) globales vmin/vmax nur aus der vorletzten Spalte (sl_in) berechnen ───\n",
    "#    abs(tgt_data) über alle x,y,z\n",
    "all_in = np.abs(tgt_data[:, :, :, t, T])\n",
    "vmin = all_in.min()\n",
    "vmax = all_in.max()\n",
    "\n",
    "# ── 2) Figure mit 4 Spalten ────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(\n",
    "    n_slices, 4,\n",
    "    figsize=(12, n_slices * 2.5),\n",
    "    constrained_layout=True\n",
    ")\n",
    "\n",
    "for z in range(n_slices):\n",
    "    # Slice‑Magnituden\n",
    "    sl_lr    = np.abs(baseline_data[:, :, z, t, T])\n",
    "    sl_deep  = np.abs(out_data     [:, :, z, t, T])\n",
    "    sl_in    = np.abs(tgt_data     [:, :, z, t, T])\n",
    "    sl_noisy = np.abs(noisy_data   [:, :, z, t, T])\n",
    "\n",
    "    # je vier Bilder mit identischer Farbskala aus sl_in\n",
    "    ax_lr, ax_deep, ax_in, ax_noisy = axes[z]\n",
    "    im0 = ax_lr   .imshow(sl_lr,    cmap='plasma', vmin=vmin, vmax=vmax)\n",
    "    im1 = ax_deep .imshow(sl_deep,  cmap='plasma', vmin=vmin, vmax=vmax)\n",
    "    im2 = ax_in   .imshow(sl_in,    cmap='plasma', vmin=vmin, vmax=vmax)  # referenziert für Colorbar\n",
    "    im3 = ax_noisy.imshow(sl_noisy, cmap='plasma', vmin=vmin, vmax=vmax)\n",
    "\n",
    "    ax_lr   .set_title(f\"low‑rank {rank_post} | z={z}, t={t}\")\n",
    "    ax_deep .set_title(f\"deep denoise      | z={z}, t={t}\")\n",
    "    ax_in   .set_title(f\"Ground Truth      | z={z}, t={t}\")\n",
    "    ax_noisy.set_title(f\"Noisy Data        | z={z}, t={t}\")\n",
    "    for ax in (ax_lr, ax_deep, ax_in, ax_noisy):\n",
    "        ax.axis('off')\n",
    "\n",
    "# ── 3) Eine gemeinsame Colorbar, ausgerichtet an der vorletzten Spalte ─────\n",
    "fig.colorbar(im2, ax=axes[:, 2], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301401be",
   "metadata": {},
   "source": [
    "# Compare spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ── FFT der Daten entlang der Zeitachse (axis=3) und Shift ────────────────────\n",
    "spec_deep  = np.fft.fft(out_data,    axis=3); spec_deep  = np.fft.fftshift(spec_deep,  axes=3)\n",
    "spec_lr    = np.fft.fft(baseline_data, axis=3); spec_lr    = np.fft.fftshift(spec_lr,    axes=3)\n",
    "spec_in    = np.fft.fft(tgt_data,     axis=3); spec_in    = np.fft.fftshift(spec_in,    axes=3)\n",
    "spec_noisy = np.fft.fft(noisy_data,   axis=3); spec_noisy = np.fft.fftshift(spec_noisy, axes=3)\n",
    "\n",
    "# ── Plot-Konfiguration ─────────────────────────────────────────────────────\n",
    "x, y, T = 9, 10, 7\n",
    "Z       = spec_noisy.shape[2]    # Anzahl der z-Slices (hier 21)\n",
    "F       = spec_noisy.shape[3]    # Anzahl der Frequenz-Bins\n",
    "freqs   = np.arange(F)\n",
    "rank    = 8                      # Rang für Low-Rank\n",
    "\n",
    "n_cols  = 2\n",
    "n_rows  = int(np.ceil(Z / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols,\n",
    "    figsize=(12, n_rows * 3),\n",
    "    sharex=True, sharey=True,\n",
    "    constrained_layout=True\n",
    ")\n",
    "\n",
    "for z in range(Z):\n",
    "    i, j = divmod(z, n_cols)\n",
    "    ax = axes[i, j]\n",
    "\n",
    "    # Magnituden extrahieren\n",
    "    mag_in    = np.abs(spec_in   [x, y, z, :, T])\n",
    "    mag_noisy = np.abs(spec_noisy[x, y, z, :, T])\n",
    "    mag_lr    = np.abs(spec_lr   [x, y, z, :, T])\n",
    "    mag_deep  = np.abs(spec_deep [x, y, z, :, T])\n",
    "\n",
    "    # vier Kurven\n",
    "    ax.plot(freqs, mag_noisy, '-', label='Noisy Input',        linewidth=1)\n",
    "    ax.plot(freqs, mag_lr,    '-', label=f'Low-Rank (r={rank})', linewidth=1)\n",
    "    ax.plot(freqs, mag_deep,  '-', label='Deep Denoising',     linewidth=1)\n",
    "    ax.plot(freqs, mag_in,    '--', label='Ground Truth',      linewidth=1)\n",
    "\n",
    "    ax.set_title(f\"z={z}\")\n",
    "    ax.grid(True, linestyle=':', alpha=0.3)\n",
    "\n",
    "    # Legende pro Subplot\n",
    "    ax.legend(fontsize='small', loc='upper right')\n",
    "\n",
    "    # Achsenbeschriftungen nur an den Rändern\n",
    "    if i == n_rows - 1:\n",
    "        ax.set_xlabel(\"Frequency bin\")\n",
    "    if j == 0:\n",
    "        ax.set_ylabel(\"Magnitude\")\n",
    "\n",
    "# Leere Subplots ausblenden (falls Z ungerade)\n",
    "for idx in range(Z, n_rows * n_cols):\n",
    "    i, j = divmod(idx, n_cols)\n",
    "    axes[i, j].axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05488449",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,T = 10, 10, 0\n",
    "\n",
    "# 2a) Deep-Denoising Spektrum\n",
    "spec_deep = np.fft.fft(out_data, axis=3)\n",
    "spec_deep = np.fft.fftshift(spec_deep, axes=3)\n",
    "\n",
    "# 2b) Noisy Input Spektrum\n",
    "spec_noisy = np.fft.fft(noisy_data, axis=3)\n",
    "spec_noisy = np.fft.fftshift(spec_noisy, axes=3)\n",
    "\n",
    "# 2c) Low-Rank Baseline Spektrum\n",
    "spec_lr = np.fft.fft(baseline_data, axis=3)\n",
    "spec_lr = np.fft.fftshift(spec_lr, axes=3)\n",
    "\n",
    "# ── 21 Spektren für z=0…20 in einem 5×5-Grid plotten ─────────────────────────\n",
    "\n",
    "# ── 21 Spektren (Noisy vs. Low-Rank vs. Noise2Void) in 2 Spalten ────────────\n",
    "\n",
    "# ── 21 Spektren in 2 Spalten mit eigener Legende pro Plot und größerer Figure ──\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameter\n",
    "x, y, T = 10, 10, 7\n",
    "Z       = spec_noisy.shape[2]   # Anzahl der z-Slices (hier 21)\n",
    "F       = spec_noisy.shape[3]   # Anzahl der Frequenz-Bins\n",
    "freqs   = np.arange(F)\n",
    "rank    = 8                     # Rang für Low-Rank\n",
    "\n",
    "# Grid-Layout: 2 Spalten, genug Zeilen\n",
    "n_cols  = 2\n",
    "n_rows  = int(np.ceil(Z / n_cols))\n",
    "\n",
    "# Figure größer machen: Breite × Höhe in Zoll\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols,\n",
    "    figsize=(12, n_rows * 3),\n",
    "    sharex=True, sharey=True,\n",
    "    constrained_layout=True\n",
    ")\n",
    "\n",
    "for z in range(Z):\n",
    "    i, j = divmod(z, n_cols)\n",
    "    ax = axes[i, j]\n",
    "\n",
    "    # Magnituden extrahieren\n",
    "    mag_noisy = np.abs(spec_noisy[x, y, z, :, T])\n",
    "    mag_lr    = np.abs(spec_lr   [x, y, z, :, T])\n",
    "    mag_deep  = np.abs(spec_deep [x, y, z, :, T])\n",
    "\n",
    "    # Plots\n",
    "    ax.plot(freqs, mag_lr,    '-', label=f'Low-Rank (r={rank_post})', linewidth=1)\n",
    "    ax.plot(freqs, mag_deep,  '-',  label=f'deep denoising', linewidth=1)\n",
    "\n",
    "    ax.set_title(f\"z={z}\")\n",
    "    ax.grid(True, linestyle=':', alpha=0.3)\n",
    "\n",
    "    # Legende für jeden Subplot\n",
    "    ax.legend(fontsize='small', loc='upper right')\n",
    "\n",
    "    # Achsenbeschriftungen nur außen\n",
    "    if i == n_rows - 1:\n",
    "        ax.set_xlabel(\"Frequency bin\")\n",
    "    if j == 0:\n",
    "        ax.set_ylabel(\"Magnitude\")\n",
    "\n",
    "# Leere Subplots ausblenden\n",
    "for idx in range(Z, n_rows * n_cols):\n",
    "    i, j = divmod(idx, n_cols)\n",
    "    axes[i, j].axis('off')\n",
    "\n",
    "#plt.savefig(\"spectra.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8999c2",
   "metadata": {},
   "source": [
    "# Residuals to full dank noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd88d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameter\n",
    "x, y, T = 10, 10, 7\n",
    "Z       = spec_noisy.shape[2]   # Anzahl der z-Slices (z=0...20)\n",
    "F       = spec_noisy.shape[3]   # Frequenz-Bins\n",
    "freqs   = np.arange(F)\n",
    "rank    = 8                     # Rang für Low-Rank\n",
    "\n",
    "# Grid-Layout: 2 Spalten (Spektrum + Residuum), viele Zeilen\n",
    "n_cols  = 2\n",
    "n_rows  = Z\n",
    "\n",
    "# Figure: größere Höhe, weil mehr Zeilen\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols,\n",
    "    figsize=(12, n_rows * 2),\n",
    "    sharex=True, constrained_layout=True\n",
    ")\n",
    "\n",
    "for z in range(Z):\n",
    "    ax_spec = axes[z, 0]\n",
    "    ax_res  = axes[z, 1]\n",
    "\n",
    "    # Magnituden extrahieren\n",
    "    mag_noisy = np.abs(spec_noisy[x, y, z, :, T])\n",
    "    mag_lr    = np.abs(spec_lr   [x, y, z, :, T])\n",
    "    mag_deep  = np.abs(spec_deep [x, y, z, :, T])\n",
    "    resid     = np.abs(spec_noisy[x, y, z, :, T] - spec_deep [x, y, z, :, T])\n",
    "\n",
    "    # Spektrenplot\n",
    "    ax_spec.plot(freqs, mag_noisy, '-', label='no denoising', linewidth=1)\n",
    "    ax_spec.plot(freqs, mag_lr,    '-', label=f'Low-Rank (r={rank})', linewidth=1)\n",
    "    ax_spec.plot(freqs, mag_deep,  '-', label=f'deep denoising', linewidth=1)\n",
    "    ax_spec.set_title(f\"z={z} – Spectra\")\n",
    "    ax_spec.grid(True, linestyle=':', alpha=0.3)\n",
    "    ax_spec.legend(fontsize='small', loc='upper right')\n",
    "\n",
    "    # Residuenplot\n",
    "    ax_res.plot(freqs, resid, '-', color='black', linewidth=1)\n",
    "    ax_res.set_title(f\"z={z} – Residuum (noisy − deep)\")\n",
    "    ax_res.grid(True, linestyle=':', alpha=0.3)\n",
    "\n",
    "    # Achsenbeschriftungen\n",
    "    if z == n_rows - 1:\n",
    "        ax_spec.set_xlabel(\"Frequency bin\")\n",
    "        ax_res.set_xlabel(\"Frequency bin\")\n",
    "    if z == 0:\n",
    "        ax_spec.set_ylabel(\"Magnitude\")\n",
    "        ax_res.set_ylabel(\"Residual\")\n",
    "\n",
    "#plt.savefig(\"spectra_and_residuals.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87ec1a",
   "metadata": {},
   "source": [
    "# Low rank on residual test:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e021ee",
   "metadata": {},
   "source": [
    "# Compare spectral peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ── Einstellungen -----------------------------------------------------------\n",
    "t, T = 101, 0\n",
    "n_slices = out_data_ft.shape[2]\n",
    "\n",
    "# ── 1) globales vmin/vmax nur aus der vorletzten Spalte (sl_in) berechnen ───\n",
    "all_in = np.abs(tgt_data_ft[:, :, :, t, T])\n",
    "vmin = all_in.min()\n",
    "vmax = all_in.max()\n",
    "\n",
    "# ── 2) Figure mit 4 Spalten ────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(\n",
    "    n_slices, 4,\n",
    "    figsize=(12, n_slices * 2.5),\n",
    "    constrained_layout=True\n",
    ")\n",
    "\n",
    "for z in range(n_slices):\n",
    "    # Slice‑Magnituden\n",
    "    sl_lr    = np.abs(baseline_data_ft[:, :, z, t, T])\n",
    "    sl_deep  = np.abs(out_data_ft     [:, :, z, t, T])\n",
    "    sl_in    = np.abs(tgt_data_ft     [:, :, z, t, T])\n",
    "    sl_noisy = np.abs(noisy_data_ft   [:, :, z, t, T])\n",
    "\n",
    "    ax_lr, ax_deep, ax_in, ax_noisy = axes[z]\n",
    "\n",
    "    im0 = ax_lr   .imshow(sl_lr,    cmap='plasma', vmin=vmin, vmax=vmax)\n",
    "    im1 = ax_deep .imshow(sl_deep,  cmap='plasma', vmin=vmin, vmax=vmax)\n",
    "    im2 = ax_in   .imshow(sl_in,    cmap='plasma', vmin=vmin, vmax=vmax)  # Referenz für Colorbar\n",
    "    im3 = ax_noisy.imshow(sl_noisy, cmap='plasma', vmin=vmin, vmax=vmax)\n",
    "\n",
    "    ax_lr   .set_title(f\"low‑rank {rank_post} | z={z}, t={t}\")\n",
    "    ax_deep .set_title(f\"deep denoise        | z={z}, t={t}\")\n",
    "    ax_in   .set_title(f\"ground truth        | z={z}, t={t}\")\n",
    "    ax_noisy.set_title(f\"noisy input         | z={z}, t={t}\")\n",
    "\n",
    "    for ax in (ax_lr, ax_deep, ax_in, ax_noisy):\n",
    "        ax.axis('off')\n",
    "\n",
    "# ── 3) Gemeinsame Colorbar, ausgerichtet an der vorletzten Spalte ─────────\n",
    "fig.colorbar(im2, ax=axes[:, 2], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb58b9f8",
   "metadata": {},
   "source": [
    "# Compare average spectra\n",
    "Here I compare the average spectrum over time (which is a high SNR estimate) for gey matter which matter and all matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_out = np.mean(out_data_ft, axis=(0, 1, 2))\n",
    "\n",
    "avg_lr = np.mean(baseline_data_ft, axis=(0, 1, 2))\n",
    "\n",
    "avg_tgt = np.mean(tgt_data_ft, axis=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c6d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Subplot-Grid: 4 Zeilen × 2 Spalten für T = 0 bis 7\n",
    "num_rows, num_cols = 4, 2\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 16))\n",
    "\n",
    "for idx, T in enumerate(range(8)):\n",
    "    i = idx // num_cols\n",
    "    j = idx % num_cols\n",
    "    ax = axes[i, j]\n",
    "    \n",
    "    # Spektren extrahieren\n",
    "    Line   = np.abs(avg_out)[:, T]\n",
    "    Line_2 = np.abs(avg_tgt)[:, T]\n",
    "    Line_3 = np.abs(avg_lr)[:, T]\n",
    "    \n",
    "    ax.plot(Line,   label=\"Deep Denoising\")\n",
    "    ax.plot(Line_2, linestyle=\"--\", label=\"Ground Truth\")\n",
    "    ax.plot(Line_3, linestyle=\":\",  label=\"Low-Rank\")\n",
    "    \n",
    "    ax.set_xlabel(\"FID-Zeitpunkt (t)\")\n",
    "    ax.set_ylabel(\"Betragsspektrum\")\n",
    "    ax.set_title(f\"T = {T}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe20bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_5d(data, rank):\n",
    "    \"\"\"\n",
    "    Computes a low-rank decomposition of a tensor with shape (22, 22, 21, 96, 8)\n",
    "    using truncated SVD.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Numpy array of shape (x, y, z, t, T).\n",
    "        rank (int): The number of singular values to keep (final rank).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The reconstructed tensor with rank 'rank'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack dimensions\n",
    "    x, y, z, t, T = data.shape\n",
    "    \n",
    "    # Reshape the 5D tensor into a 2D matrix of shape (x*y*z, t*T)\n",
    "    # Use 'F' (Fortran) order to match MATLAB's column-major ordering\n",
    "    reshaped_matrix = data.reshape((x * y * z * T, t), order='F')\n",
    "    \n",
    "    # Perform economy-size SVD (similar to MATLAB's \"svd(..., 'econ')\")\n",
    "    U, singular_values, Vh = np.linalg.svd(reshaped_matrix, full_matrices=False)\n",
    "    \n",
    "    # Truncate the singular values to the desired rank\n",
    "    k = min(rank, len(singular_values))  # safeguard: rank cannot exceed # of singular values\n",
    "    singular_values_truncated = np.zeros_like(singular_values)\n",
    "    singular_values_truncated[:k] = singular_values[:k]\n",
    "    \n",
    "    # Form the diagonal matrix of truncated singular values\n",
    "    S_truncated = np.diag(singular_values_truncated)\n",
    "    \n",
    "    # Reconstruct the matrix using the truncated SVD components\n",
    "    reconstructed_matrix = U @ S_truncated @ Vh\n",
    "    \n",
    "    # Reshape back to the original 5D shape, again using 'F' order\n",
    "    reconstructed_tensor = reconstructed_matrix.reshape((x, y, z, t, T), order='F')\n",
    "    \n",
    "    return reconstructed_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack dimensions\n",
    "x, y, z, t, T = out_data.shape\n",
    "\n",
    "# Reshape the 5D tensor into a 2D matrix of shape (x*y*z, t*T)\n",
    "# Use 'F' (Fortran) order to match MATLAB's column-major ordering\n",
    "reshaped_matrix = tgt_data_ft.reshape((x * y * z * T, t), order='F')\n",
    "\n",
    "# Perform economy-size SVD (similar to MATLAB's \"svd(..., 'econ')\")\n",
    "U, singular_values, Vh = np.linalg.svd(reshaped_matrix, full_matrices=False)\n",
    "\n",
    "# Reshape the 5D tensor into a 2D matrix of shape (x*y*z, t*T)\n",
    "# Use 'F' (Fortran) order to match MATLAB's column-major ordering\n",
    "reshaped_matrix_raw = noisy_data_ft.reshape((x * y * z * T, t), order='F')\n",
    "\n",
    "# Perform economy-size SVD (similar to MATLAB's \"svd(..., 'econ')\")\n",
    "U, singular_values_raw, Vh = np.linalg.svd(reshaped_matrix_raw, full_matrices=False)\n",
    "\n",
    "# Scatterplot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(range(len(singular_values)), singular_values, \n",
    "            marker='o', label='singular_values')\n",
    "plt.scatter(range(len(singular_values_raw)), singular_values_raw, \n",
    "            marker='x', label='singular_values_raw')\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Wert')\n",
    "plt.title('Scatterplot der Singulärwerte')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "singular_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e088582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(range(len(singular_values)), singular_values, \n",
    "            marker='o', label='singular_values_deep')\n",
    "plt.scatter(range(len(singular_values_raw)), singular_values_raw, \n",
    "            marker='x', label='singular_values_noisy_data')\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Wert')\n",
    "plt.title('Scatterplot singular values (x*y*z*T,t)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
